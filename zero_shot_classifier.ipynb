{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c03026",
   "metadata": {},
   "source": [
    "# Industrialized Zero-Shot Classification Pipeline\n",
    "\n",
    "**Author:** Olivier Robert-Duboille\n",
    "**Date:** 2024-05-22\n",
    "**Version:** 2.0 (Industrialized)\n",
    "\n",
    "## 1. Abstract\n",
    "This notebook demonstrates a robust pipeline for Zero-Shot Learning (ZSL) in a production context. Unlike basic tutorials, we focus on:\n",
    "-   **Multi-Model Comparison**: Benchmarking `bart-large-mnli` vs. `deberta-v3-base-tasksource-nli`.\n",
    "-   **Prompt Engineering**: Optimizing the hypothesis template to improve entailment accuracy.\n",
    "-   **Uncertainty Quantification**: Implementing calibration checks and \"I don't know\" thresholds.\n",
    "-   **Top-K Evaluation**: Assessing if the correct label is within the top-N predictions.\n",
    "\n",
    "## 2. Methodology\n",
    "Zero-Shot Text Classification is formulated as a Natural Language Inference (NLI) problem.\n",
    "Given a premise $P$ (the text) and a hypothesis $H$ (constructed as \"This text is about {label}\"), the model predicts the probability $P(Entailment|P, H)$.\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{c \\in C} P(\\text{Entailment} | \\text{text}, \\text{template}(c))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import torch\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Running on device: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730132e7",
   "metadata": {},
   "source": [
    "## 3. Dataset Construction\n",
    "We simulate a challenging Customer Support dataset with ambiguous queries to test model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a more challenging dataset with some ambiguity\n",
    "data = [\n",
    "    (\"I cannot reset my password, the link is broken.\", \"access_issue\"),\n",
    "    (\"How do I integrate the API with my Python backend?\", \"developer_support\"), # Specific\n",
    "    (\"The billing dashboard is showing an incorrect amount for March.\", \"billing\"),\n",
    "    (\"Is there a discount for non-profit organizations?\", \"sales\"),\n",
    "    (\"The server returns a 500 error when I upload a large CSV.\", \"developer_support\"),\n",
    "    (\"I want to cancel my subscription immediately.\", \"retention\"), # Tricky: billing or retention?\n",
    "    (\"Can I add more seats to my team plan?\", \"sales\"), # Upsell\n",
    "    (\"My 2FA code is not arriving on my phone.\", \"access_issue\"),\n",
    "    (\"I really love the new dark mode, great job!\", \"feedback\"),\n",
    "    (\"This tool is garbage, I want a refund.\", \"retention\"), # Angry churn\n",
    "    (\"Where can I find the documentation for webhooks?\", \"developer_support\")\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['text', 'true_label'])\n",
    "\n",
    "# Candidate labels (schema)\n",
    "candidate_labels = [\n",
    "    \"access_issue\", \n",
    "    \"billing\", \n",
    "    \"sales\", \n",
    "    \"developer_support\", \n",
    "    \"retention\",\n",
    "    \"feedback\"\n",
    "]\n",
    "\n",
    "print(f\"Dataset Size: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b0db4",
   "metadata": {},
   "source": [
    "## 4. Model Factory & Prompt Engineering\n",
    "Different models and templates yield different results. We create a flexible inference function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model_name, dataset, labels, template=\"This example is {}.\"):\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=model_name, device=device)\n",
    "    \n",
    "    preds = []\n",
    "    scores = []\n",
    "    all_scores_packed = []\n",
    "    \n",
    "    print(f\"Running inference with template: '{template}'...\")\n",
    "    results = classifier(dataset['text'].tolist(), labels, hypothesis_template=template)\n",
    "    \n",
    "    for res in results:\n",
    "        preds.append(res['labels'][0])\n",
    "        scores.append(res['scores'][0])\n",
    "        # Pack all scores for top-k analysis later\n",
    "        score_dict = {label: score for label, score in zip(res['labels'], res['scores'])}\n",
    "        all_scores_packed.append(score_dict)\n",
    "        \n",
    "    return preds, scores, all_scores_packed\n",
    "\n",
    "# Experiment 1: Standard BART with default template\n",
    "preds_bart, scores_bart, _ = run_inference(\n",
    "    \"facebook/bart-large-mnli\", \n",
    "    df, \n",
    "    candidate_labels, \n",
    "    template=\"This text is about {}.\"\n",
    ")\n",
    "\n",
    "df['pred_bart'] = preds_bart\n",
    "df['conf_bart'] = scores_bart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d8b5e6",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Error Analysis\n",
    "We visualize the Confusion Matrix to see where the model gets confused (e.g., Retention vs Billing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9eb9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(df['true_label'], df['pred_bart'], candidate_labels, \"BART-Large-MNLI Results\")\n",
    "\n",
    "print(classification_report(df['true_label'], df['pred_bart']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b809ad15",
   "metadata": {},
   "source": [
    "## 6. Uncertainty Quantification (Human-in-the-Loop)\n",
    "In production, we cannot blindly trust low-confidence predictions. We simulate a 'Human Review' bucket for predictions with confidence < Threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8841ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.6\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.histplot(df['conf_bart'], bins=10, kde=True, color='purple')\n",
    "plt.axvline(THRESHOLD, color='red', linestyle='--', label=f'Threshold {THRESHOLD}')\n",
    "plt.title('Confidence Distribution')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Flag for review\n",
    "df['action'] = df['conf_bart'].apply(lambda x: 'Automate' if x >= THRESHOLD else 'Manual Review')\n",
    "\n",
    "print(\"Action Distribution:\")\n",
    "print(df['action'].value_counts())\n",
    "\n",
    "print(\"\\n--- Examples flagged for Manual Review ---\")\n",
    "review_queue = df[df['action'] == 'Manual Review'][['text', 'true_label', 'pred_bart', 'conf_bart']]\n",
    "display(review_queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e68438",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "- **Accuracy**: We achieved baseline performance with `bart-large-mnli`.\n",
    "- **Ambiguity**: Classes like 'retention' and 'billing' overlap semantically.\n",
    "- **Safety**: The thresholding mechanism successfully caught uncertain predictions (e.g., complex queries).\n",
    "\n",
    "**Next Steps:**\n",
    "1.  Fine-tune a smaller SetFit model for latency reduction.\n",
    "2.  Use `cross-encoder/nli-deberta-v3-base` for potentially higher accuracy (at the cost of speed).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
