{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Text Classification with Transformers\n",
    "\n",
    "**Author:** Olivier Robert-Duboille\n",
    "\n",
    "## 1. Introduction\n",
    "Zero-Shot Learning (ZSL) allows a model to classify data into classes it has never seen during training. This is particularly powerful in NLP, where we can leverage large pre-trained language models (LLMs) to understand the semantic relationship between a text sequence and candidate labels.\n",
    "\n",
    "### Objectives:\n",
    "- Implement a Zero-Shot Classifier pipeline using Hugging Face `transformers`.\n",
    "- Create a custom dataset of \"unseen\" topics (e.g., specific legal or medical queries).\n",
    "- Evaluate the model's ability to categorize these queries without any fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Model\n",
    "We will use the `bart-large-mnli` model, which is pre-trained on the Multi-Genre Natural Language Inference (MNLI) corpus. It treats classification as an entailment problem: Given a premise (text) and a hypothesis (This text is about {label}), does the premise entail the hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the zero-shot classification pipeline\n",
    "# Note: This will download the model weights (~1.6GB) if not cached.\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a \"Mock\" Unseen Dataset\n",
    "Let's simulate a scenario where we are analyzing customer support tickets for a Tech SaaS company. The model has not been explicitly trained on these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"I cannot reset my password, the link is broken.\", \"access_issue\"),\n",
    "    (\"How do I integrate the API with my Python backend?\", \"technical_support\"),\n",
    "    (\"The billing dashboard is showing an incorrect amount for March.\", \"billing\"),\n",
    "    (\"Is there a discount for non-profit organizations?\", \"sales\"),\n",
    "    (\"The server returns a 500 error when I upload a large CSV.\", \"technical_support\"),\n",
    "    (\"I want to cancel my subscription immediately.\", \"billing\"),\n",
    "    (\"Can I add more seats to my team plan?\", \"sales\"),\n",
    "    (\"My 2FA code is not arriving on my phone.\", \"access_issue\")\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['text', 'true_label'])\n",
    "candidate_labels = [\"access_issue\", \"billing\", \"sales\", \"technical_support\", \"feature_request\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference\n",
    "We will now run the classifier on our dataset. The model outputs a probability distribution over the candidate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "confidence_scores = []\n",
    "\n",
    "print(f\"Classifying {len(df)} examples...\")\n",
    "\n",
    "for text in df['text']:\n",
    "    result = classifier(text, candidate_labels)\n",
    "    # The result is sorted by score, so the first element is the top prediction\n",
    "    top_label = result['labels'][0]\n",
    "    top_score = result['scores'][0]\n",
    "    \n",
    "    predictions.append(top_label)\n",
    "    confidence_scores.append(top_score)\n",
    "\n",
    "df['predicted_label'] = predictions\n",
    "df['confidence'] = confidence_scores\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Analysis\n",
    "Let's see how well our zero-shot model performed without any training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(df['true_label'], df['predicted_label'], labels=candidate_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=candidate_labels, yticklabels=candidate_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Zero-Shot Classification Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(df['true_label'], df['predicted_label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confidence Analysis\n",
    "It's useful to understand how confident the model is. Low confidence predictions might require human review in a production system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df['confidence'], bins=10, kde=True, color='purple')\n",
    "plt.title('Distribution of Model Confidence Scores')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.show()\n",
    "\n",
    "# Show low confidence examples\n",
    "threshold = 0.7\n",
    "low_conf = df[df['confidence'] < threshold]\n",
    "if not low_conf.empty:\n",
    "    print(f\"Examples with confidence < {threshold}:\")\n",
    "    print(low_conf[['text', 'predicted_label', 'confidence']])\n",
    "else:\n",
    "    print(f\"All predictions have confidence >= {threshold}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}